{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est intéressant de constater qu'en apprentissage profond, l'évolution des architecture et des techniques est très rapide, mais que toutefois certains aspects peuvent rester des années présent dans l'ensemble des codes, sans savoir vraiment s'ils sont totalement nécessaire.\n",
    "\n",
    "Un de ces éléments et la normalisation qui est utilisé en entrée de la plupart des réseaux convolutionnels qui servent à traiter des images visuelles. Ceci est d'autant plus vrai que nous utilisons souvent dans la communauté des réseaux qui sont pré entraînés pour des images normalisé à des valeurs données.\n",
    "Ceci est d'autant plus vrai que nous utilisons souvent dans la communauté des réseaux qui sont pré entraînés pour des images normalisé à des valeurs données.\n",
    "\n",
    "Je m'intéresse ici de savoir si ces réseaux restent efficaces quand on change la normalisation en entrée. En effet, comme les poids de la première couche convolutionnelle sont appris, et que notamment grâce au biais dans les convolutions ainsi que les noyaux de convolution puissent être multiplié par des valeurs arbitraires, une normalisation vers une certaine valeur n'a a priori aucun sens, et simplement le fait de normaliser en a un. Je me propose ici de montrer quantitativement que c'est le cas, d'abord pour un réseau *historique* LeNet appliqué au challenge MNIST, et ensuite à ResNet appliqué à ImageNet.\n",
    "\n",
    "\n",
    "<!-- TEASER_END -->\n",
    "\n",
    "Let's first initialize the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "import os\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format='retina'\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import matplotlib.pyplot as plt\n",
    "phi = (np.sqrt(5)+1)/2\n",
    "fig_width = 10\n",
    "figsize = (fig_width, fig_width/phi)\n",
    "from IPython.display import display, HTML\n",
    "def show_video(filename): \n",
    "    return HTML(data='<video src=\"{}\" loop autoplay width=\"600\" height=\"600\"></video>'.format(filename))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "\n",
    "Explorons d'abord, le réseau *leNet*, dont l'objectif est de catégoriser, des images de lettres dactylographiées, une des premières grandes réussites des réseaux de neurones Multicouche. Nous allons pour celui-là utiliser l'implémentation classique, tel qu'elle est proposé dans la série des exemples de la librairie PyTorch. \n",
    "\n",
    "La cellule ci-dessous permet de tout faire : d'abord charger les librairies, ensuite définir le réseau de neurones, et enfin définir la procédure d'apprentissage et de test qui sont appliquées à la base de données. On obtient en sortie une valeur d'Accura qui correspond au pourcentage de lettres qui sont correctement classifier.\n",
    ".\n",
    "\n",
    "Dans cette cellule, on va isoler les deux paramètres qui vont permettre de régler la moyenne ainsi que la déviation standard qui sont appliquée à la fonction de normalisation, et que nous allons manipuler au cours de ce book\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://raw.githubusercontent.com/pytorch/examples/refs/heads/main/mnist/main.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval=100, verbose=True):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if verbose and batch_idx % log_interval  == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, verbose=True):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    if verbose:\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return correct / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "def main(mean, std, epochs=15, log_interval=100, verbose=True):\n",
    "    # Training settings\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    use_mps = torch.backends.mps.is_available()\n",
    "\n",
    "    torch.manual_seed(1998) # FOOTIX rules !\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif use_mps:\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': 64}\n",
    "    test_kwargs = {'batch_size': 1000}\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((mean,), (std,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "\n",
    "    dataset2 = datasets.MNIST('../data', train=False,\n",
    "                       transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "    for epoch in range(1, epochs):\n",
    "        train(model, device, train_loader, optimizer, epoch, log_interval=log_interval, verbose=verbose)\n",
    "        scheduler.step()\n",
    "\n",
    "    accuracy = test(model, device, test_loader, verbose=verbose)\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons défini l'ensemble du protocole, nous avons pouvoir le tester tout d'abord dans sa formule la plus classique, telle qu'elle est livrée dans la librairie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = main(mean=0.1307, std=0.3081)\n",
    "print(f'{accuracy=:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un avantage de notre code et maintenant de pouvoir manipuler ces deux valeurs, afin de voir si en partant du même code on obtient une valeur d'accuracy qui est différente.\n",
    "\n",
    "\n",
    "Le résultat semble similaire, mais ce n'est pas assez pour démontrer que la moyenne est la déviation standard n'ont pas d'effets sur l'apprentissage. Je vais maintenant utiliser les grands moyens en utilisant une librairie qui permet de tester de multiples valeurs et ainsi d'optimiser les paramètres. Si la moyenne est la déviation sont si important. Alors nous allons converger vers un ensemble de valeur fixe, alors que si c'est moins important, les valeurs pourront être assez dispersées. Cela nous permettra en particulier de choisir une valeur arbitraire comme par exemple une moyenne de zéro et une déviation standard de zéro, ce qu'on appelle une normalisation *normale*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=0.9909\n"
     ]
    }
   ],
   "source": [
    "accuracy = main(mean=0., std=1., epochs=epochs, verbose=False)\n",
    "print(f'{accuracy=:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "path_save_optuna =  'optuna-MNIST.sqlite3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm {path_save_optuna}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941fe06941cb434fa3cdd08bc49c72af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    mean = trial.suggest_float('mean', 0.001, 10, log=True)\n",
    "    std = trial.suggest_float('std', 0.001, 10, log=True)\n",
    "    accuracy = main(mean=mean, std=std, epochs=epochs, verbose=False)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "if not(os.path.isfile(path_save_optuna)):\n",
    "    study = optuna.create_study(direction='maximize', load_if_exists=True, \n",
    "                                storage=f\"sqlite:///{path_save_optuna}\", study_name='MNIST')\n",
    "    study.optimize(objective, n_trials=200, n_jobs=-1, show_progress_bar=True)\n",
    "    print(50*'-.')\n",
    "    print(\"Best params: \", study.best_params)\n",
    "    print(\"Best value: \", study.best_value)\n",
    "    print(\"Best Trial: \", study.best_trial)\n",
    "    print(50*'=')\n",
    "    # print(\"Trials: \", study.trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://optuna.readthedocs.io/en/stable/reference/visualization/matplotlib/generated/optuna.visualization.matplotlib.contour.html\n",
    "from optuna.visualization.matplotlib import plot_contour\n",
    "\n",
    "fig = plot_contour(study, params=[\"mean\", \"std\"], target_name=\"Accuracy\")\n",
    "fig.set_size_inches(figsize)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some book keeping for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -i -h -m -v -p numpy,matplotlib,torch  -r -g -b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
